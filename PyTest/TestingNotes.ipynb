{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "X3HY7rslTJW3",
        "24e1f35e",
        "ZQ_qY23HM-G_",
        "auqcll5JSHnR",
        "l3gQJLiFR9wd",
        "ubQPdTtamZ7O",
        "h95MZngDal5q",
        "fOLF0Exb9TO7",
        "iTBGkKxp9XJX",
        "LXjFUjFbR-bC"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24e1f35e"
      },
      "source": [
        "### Write a unit test for registration page using pytest\n",
        "\n",
        "Write a pytest test function that calls a dummy registration function with different inputs (valid and invalid) and asserts the expected outcomes. Use the `%%writefile` magic command to save the dummy registration function and the unit tests to a Python file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4996870",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c752007-e3fd-47f4-defe-ebaf2671c3b3"
      },
      "source": [
        "%%writefile test_registration.py\n",
        "import pytest\n",
        "\n",
        "def register_user(username, email, password):\n",
        "    if not username or not email or not password:\n",
        "        return False\n",
        "    if \"@\" not in email:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def test_registration():\n",
        "    # Test with valid inputs\n",
        "    assert register_user(\"testuser\", \"test@example.com\", \"password123\") == True\n",
        "\n",
        "    # Test with invalid inputs\n",
        "    assert register_user(\"\", \"test@example.com\", \"password123\") == False  # Missing username\n",
        "    assert register_user(\"testuser\", \"\", \"password123\") == False  # Missing email\n",
        "    assert register_user(\"testuser\", \"test@example.com\", \"\") == False  # Missing password\n",
        "    assert register_user(\"testuser\", \"testexample.com\", \"password123\") == False  # Invalid email format"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test_registration.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10479239"
      },
      "source": [
        "Execute the tests using the `!pytest` command.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "389dd80a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f20e618-0148-4a12-8048-98723ab5da80"
      },
      "source": [
        "!pytest"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.11, pytest-8.4.2, pluggy-1.6.0\n",
            "rootdir: /content\n",
            "plugins: langsmith-0.4.28, typeguard-4.4.4, anyio-4.10.0\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 1 item                                                               \u001b[0m\n",
            "\n",
            "test_registration.py \u001b[32m.\u001b[0m\u001b[32m                                                   [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unit Testing a Calculator"
      ],
      "metadata": {
        "id": "ZQ_qY23HM-G_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test_calculator.py\n",
        "\n",
        "import pytest\n",
        "\n",
        "def add(num1, num2): return num1 + num2\n",
        "def subtract(num1, num2): return abs(num1 - num2)\n",
        "def multiply(num1, num2): return num1 * num2\n",
        "def divide(num1, num2):\n",
        "  try: return num1 / num2\n",
        "  except ZeroDivisionError: return \"Cannot divide by zero\"\n",
        "\n",
        "def test_calculator():\n",
        "  assert add(2, 3) == 5\n",
        "  assert subtract(5, 3) == 2\n",
        "  assert multiply(2, 3) == 6\n",
        "  assert divide(6, 4) == 1.5\n",
        "\n",
        "  assert add(2, 3) != -1\n",
        "  assert subtract(5, -3) != -8\n",
        "  assert multiply(2, -3) != 6\n",
        "  assert divide(6, 2) != \"Cannot divide by zero\""
      ],
      "metadata": {
        "id": "UewBFuYCJX1S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47a0f781-d0cb-4dd5-fd5e-a3b74c5bdd12"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test_calculator.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Writing a unit test for password strength using PyTest"
      ],
      "metadata": {
        "id": "auqcll5JSHnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test_password_strength.py\n",
        "\n",
        "import pytest\n",
        "import re        # regular expressions package\n",
        "\n",
        "def validate_password(password):\n",
        "\n",
        "    if len(password) < 8 or len(password) > 16: return False             # Password length\n",
        "    if \" \" in password or \"-\" in password: return False                  # No space or hyphen in PW\n",
        "    if not re.search(r'\\d', password): return False                      # at least 1 number\n",
        "    if not re.search(r'[!@#$%^&*(),.?\":{}|<>]', password): return False  # at least 1 special char\n",
        "    if not re.search(r'[A-Z]', password): return False                   # at least 1 uppercase letter\n",
        "    if not re.search(r'[a-z]', password): return False                   # at least 1 lower case letter\n",
        "\n",
        "    return True\n",
        "\n",
        "def test_password_strength():\n",
        "    # Test with valid password\n",
        "    assert validate_password(\"Password123!\") == True\n",
        "    assert validate_password(\"aB1@cDeFghIjK\") == True\n",
        "\n",
        "    # Test with invalid passwords\n",
        "    assert validate_password(\"short\") == False                   # Too short\n",
        "    assert validate_password(\"toolongpasswordtoolong\") == False  # Too long\n",
        "    assert validate_password(\"Password 123!\") == False           # Contains space\n",
        "    assert validate_password(\"Password-123!\") == False           # Contains hyphen\n",
        "    assert validate_password(\"Password!!!\") == False             # Missing number\n",
        "    assert validate_password(\"Password123\") == False             # Missing special character\n",
        "    assert validate_password(\"password123!\") == False            # Missing uppercase letter\n",
        "    assert validate_password(\"PASSWORD123!\") == False            # Missing lowercase letter\n",
        "    assert validate_password(\"12345678!\") == False               # Missing upper and lower case\n",
        "    assert validate_password(\"ABCDEFG@\") == False                # Missing number and lower case"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtDtfcaySUfQ",
        "outputId": "8cce41b0-2aa2-4c74-8f48-59abcbf72225"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test_password_strength.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest"
      ],
      "metadata": {
        "id": "M1vRNB0qUV6U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef4ac22a-99c9-47b8-a452-4ec0f269cff2"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.11, pytest-8.4.2, pluggy-1.6.0\n",
            "rootdir: /content\n",
            "plugins: langsmith-0.4.28, typeguard-4.4.4, anyio-4.10.0\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 3 items                                                              \u001b[0m\n",
            "\n",
            "test_calculator.py \u001b[32m.\u001b[0m\u001b[32m                                                     [ 33%]\u001b[0m\n",
            "test_password_strength.py \u001b[32m.\u001b[0m\u001b[32m                                              [ 66%]\u001b[0m\n",
            "test_registration.py \u001b[32m.\u001b[0m\u001b[32m                                                   [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m============================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.04s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parametrized fixture"
      ],
      "metadata": {
        "id": "l3gQJLiFR9wd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test_param.py\n",
        "\n",
        "import pytest\n",
        "\n",
        "@pytest.mark.parametrize(\"x, y, result\", [(a,b,a+b) for a in range(1,11) for b in range(10,0,-1)])\n",
        "def test_add(x, y, result): assert x + y == result"
      ],
      "metadata": {
        "id": "ji1agzNENFbG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5126ff09-0177-4ef7-d510-a7444c14d854"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test_param.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Here's why parameterized fixtures are great:"
      ],
      "metadata": {
        "id": "ubQPdTtamZ7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test_sum_loop.py\n",
        "\n",
        "import pytest\n",
        "\n",
        "# def add(a, b):\n",
        "#   return a + b\n",
        "\n",
        "# def test_sum_loop():\n",
        "#   data = [\n",
        "#       [1, 2, 3],    # any change in these input vals will fail the entire test (remaining inputs aren't tested) and\n",
        "#       [-1, 1, 0],   # we won't know which input vals (and which loop cycle) resulted in failure (only assertion error is raised)\n",
        "#       [0, 0, 0]     # that's why --> paramterised fixtures --> on failure, you know which input vals caused it\n",
        "#   ]                 # and the remaining input vals will carry on with the run\n",
        "#   for x, y, ttl in data:\n",
        "#     s = add(x, y)\n",
        "#     assert s == ttl\n",
        "\n",
        "'''Here is how parametrized fixtures are used: '''\n",
        "data = [\n",
        "      [1, 2, 3],\n",
        "      [-1, 1, 0],\n",
        "      [0, 0, 0]\n",
        "  ]\n",
        "\n",
        "@pytest.mark.parametrize(\"x,y,ttl\", data)\n",
        "def test_sum_parameterized(x, y, ttl):\n",
        "  # s = add(x, y)\n",
        "  assert x + y == ttl"
      ],
      "metadata": {
        "id": "9QlO7lrFjAvL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc330d0c-bf67-441c-d599-5a0003769a6d"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test_sum_loop.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Delete test file"
      ],
      "metadata": {
        "id": "nNngkbFIw09q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -f /content/test_sum_loop.py"
      ],
      "metadata": {
        "id": "HVEGaKqiDnSZ"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## freeCodeCamp PyTest Tutorial ([YouTube Link](https://https://www.youtube.com/watch?v=cHYq1MRoyI0))"
      ],
      "metadata": {
        "id": "cIx_BoXQyFx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Registering markers in a .ini file.\n",
        "# # This file should be present in root directory.\n",
        "# # Enables one to run only one type of test at console (e.g. only smoke, or only sanity, etc.)\n",
        "\n",
        "# %%writefile pytest.ini\n",
        "\n",
        "# # pytest.ini\n",
        "# [pytest]\n",
        "# markers =\n",
        "#     sanity: marks tests as sanity tests\n",
        "#     smoke: marks tests as smoke tests"
      ],
      "metadata": {
        "id": "r0d-1QRP00YY"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing division"
      ],
      "metadata": {
        "id": "fOLF0Exb9TO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test_div.py\n",
        "import pytest\n",
        "\n",
        "# @pytest.mark.smoke       # run only 'smoke' test : pytest -m smoke test_fileName.py\n",
        "# def test_demo(): pass\n",
        "\n",
        "# @pytest.mark.sanity      # run only 'sanity' test : pytest -m sanity test_fileName.py\n",
        "# def test_demo2(): pass\n",
        "\n",
        "def divide(num1, num2):\n",
        "  return num1 / num2\n",
        "\n",
        "def test_divide_by_zero(): assert divide(4, 2)  # will be True"
      ],
      "metadata": {
        "id": "lbQnhlxFyJ4B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36bb4b63-03a8-407a-cabf-50528a629c43"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test_div.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing division by zero"
      ],
      "metadata": {
        "id": "iTBGkKxp9XJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test_div_zero.py\n",
        "import pytest\n",
        "\n",
        "def divide(num1, num2):\n",
        "  # if num2 == 0: raise ValueError # if this line is commented, ZeroDivisionError line below will run\n",
        "  return num1 / num2\n",
        "\n",
        "def test_divide_by_zero():\n",
        "  with pytest.raises(ZeroDivisionError): # 'ZeroDivisionError' line\n",
        "    assert divide(4, 0)\n"
      ],
      "metadata": {
        "id": "3aTH6OE99Apo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d578d76-970d-4049-c8ca-a3bc314eb9f3"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test_div_zero.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing String Concatenation"
      ],
      "metadata": {
        "id": "LXjFUjFbR-bC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test_string_concat.py\n",
        "\n",
        "def concat_strings(str1, str2): return str1 + str2\n",
        "\n",
        "def test_concat_strings(): assert concat_strings(\"Hello\", \" World\") == \"Hello World\""
      ],
      "metadata": {
        "id": "8--jycv4SBxe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c06e8eac-6926-48dd-e995-03282be3b6fb"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test_string_concat.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Class based tests"
      ],
      "metadata": {
        "id": "z7Ufr2GdSpDg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Open Terminal and type the following commands in order:\n",
        "\n",
        "```\n",
        "mkdir tests\n",
        "cd tests\n",
        "```\n",
        "\n",
        "Command ```mkdir tests``` will create a folder ```tests``` and command ```cd tests``` will take you inside the folder ```tests``` where you'll create files.\n",
        "\n",
        "Also, in Terminal, ensure that the ```pytest test_fileName.py``` command is run in directory ```tests```."
      ],
      "metadata": {
        "id": "9G4qqGQhcpjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile tests/shapes.py\n",
        "\n",
        "import math\n",
        "\n",
        "class Shape:\n",
        "  def area(): pass\n",
        "  def perimeter(): pass\n",
        "\n",
        "class Circle(Shape):   # creating 'Circle' obj of parent type 'Shape'\n",
        "  def __init__(self, radius): self.radius = radius\n",
        "  def area(self): return math.pi * self.radius ** 2\n",
        "  def perimeter(self): return 2 * math.pi * self.radius\n",
        "\n",
        "# this file can be run in a new cell via : !pytest tests/shapes.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5DFSq0aSsZI",
        "outputId": "63e47aa0-9ed6-465c-dec9-8ba2432c8789"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing tests/shapes.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class based tests provide us with setup method (runs before each test) and teardown method (runs after each test).\n",
        "\n",
        "Here is a demo ahead:"
      ],
      "metadata": {
        "id": "K4z8MXHsVT_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile tests/test_setupTeardown.py\n",
        "\n",
        "import pytest\n",
        "\n",
        "class TestCircle:\n",
        "  def setup_method(self, method): print(f\"\\n\\nSetting up {method}\\n\")  # fn will run before each test's run\n",
        "  def teardown_method(self, method): print(f\"Tearing down {method}\")   # fn will run after each test's run\n",
        "  def test_something1(self): # at least one test fn needed above two prints won't display in next cell's o/p\n",
        "    print(\"Something1 runs post setup...\\n\")\n",
        "  def test_something2(self):\n",
        "    print(\"Something2 runs post setup...\\n \")"
      ],
      "metadata": {
        "id": "YSN8qFnW39ak",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4119f52d-735f-4a5f-9b6e-5b6da0b867c5"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing tests/test_setupTeardown.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest -s tests/test_setupTeardown.py  # '-s' enables display of 'print()' content"
      ],
      "metadata": {
        "id": "t_RwJz_7p4Rg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a75a96b5-3a37-4aac-d5b9-2ccea435e030"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.11, pytest-8.4.2, pluggy-1.6.0\n",
            "rootdir: /content\n",
            "plugins: langsmith-0.4.28, typeguard-4.4.4, anyio-4.10.0\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 2 items                                                              \u001b[0m\n",
            "\n",
            "tests/test_setupTeardown.py \n",
            "\n",
            "Setting up <bound method TestCircle.test_something1 of <test_setupTeardown.TestCircle object at 0xfe722b968d0>>\n",
            "\n",
            "Something1 runs post setup...\n",
            "\n",
            "\u001b[32m.\u001b[0mTearing down <bound method TestCircle.test_something1 of <test_setupTeardown.TestCircle object at 0xfe722b968d0>>\n",
            "\n",
            "\n",
            "Setting up <bound method TestCircle.test_something2 of <test_setupTeardown.TestCircle object at 0xfe722b95100>>\n",
            "\n",
            "Something2 runs post setup...\n",
            " \n",
            "\u001b[32m.\u001b[0mTearing down <bound method TestCircle.test_something2 of <test_setupTeardown.TestCircle object at 0xfe722b95100>>\n",
            "\n",
            "\n",
            "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile tests/test_circle.py\n",
        "\n",
        "import pytest\n",
        "import math\n",
        "import shapes as shapes\n",
        "\n",
        "class TestCircle:\n",
        "  def setup_method(self, method):  # fn will run before each test's run\n",
        "    print(f\"\\n\\nSetting up {method}.\")\n",
        "    self.circle = shapes.Circle(10)  # 'shapes.py' file has 'Circle' child class of parent class 'Shape'\n",
        "\n",
        "  def teardown_method(self, method): # fn will run after each test's run\n",
        "    print(f\"Tearing down {method}\")\n",
        "    del self.circle\n",
        "\n",
        "  def test_area(self) -> float:\n",
        "    print(\"Asserting area of circle :\", math.pi * self.circle.radius ** 2)\n",
        "    assert self.circle.area() == math.pi * self.circle.radius ** 2\n",
        "\n",
        "  def test_perimeter(self) -> float:\n",
        "    print(\"Asserting perimeter of circle: \", math.pi * self.circle.radius)\n",
        "    assert self.circle.perimeter() == 2 * math.pi * self.circle.radius\n"
      ],
      "metadata": {
        "id": "2A14EyksWrGZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51f9788a-3669-46cb-cfa3-ed4dcccd6039"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing tests/test_circle.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest -s tests/test_circle.py   # '-s' enables display of 'print()' content"
      ],
      "metadata": {
        "id": "_Z2hPV9cKg74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c84d9eff-5025-403b-8e16-b5842ea55eb5"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.11, pytest-8.4.2, pluggy-1.6.0\n",
            "rootdir: /content\n",
            "plugins: langsmith-0.4.28, typeguard-4.4.4, anyio-4.10.0\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 2 items                                                              \u001b[0m\n",
            "\n",
            "tests/test_circle.py \n",
            "\n",
            "Setting up <bound method TestCircle.test_area of <test_circle.TestCircle object at 0x10387eeff590>>.\n",
            "Asserting area of circle : 314.1592653589793\n",
            "\u001b[32m.\u001b[0mTearing down <bound method TestCircle.test_area of <test_circle.TestCircle object at 0x10387eeff590>>\n",
            "\n",
            "\n",
            "Setting up <bound method TestCircle.test_perimeter of <test_circle.TestCircle object at 0x10387fed1700>>.\n",
            "Asserting perimeter of circle:  31.41592653589793\n",
            "\u001b[32m.\u001b[0mTearing down <bound method TestCircle.test_perimeter of <test_circle.TestCircle object at 0x10387fed1700>>\n",
            "\n",
            "\n",
            "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fixtures"
      ],
      "metadata": {
        "id": "NJDm0PHBOBlO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fixtures can be called as functions that create/prepare the context for the running of test functions. Fixtures are run by pytest before (& sometimes after) the actual test functions. They can get a data set for the tests to work on.\n",
        "\n",
        "PyTest looks at specific name of the args within test function & then searches for a fixture with the same name. We never call fixture functions directly (PyTest does it for us).\n",
        "\n"
      ],
      "metadata": {
        "id": "l2KJzX6uda4-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us add a new class for a rectangle in ```shapes.py``` file."
      ],
      "metadata": {
        "id": "lkWpzswYsmWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile tests/shapes.py\n",
        "\n",
        "import math\n",
        "\n",
        "class Shape:\n",
        "  def area(): pass\n",
        "  def perimeter(): pass\n",
        "\n",
        "class Circle(Shape):   # creating 'Circle' obj of parent type 'Shape'\n",
        "  def __init__(self, radius: float) -> float: self.radius = radius\n",
        "  def area(self) -> float: return math.pi * self.radius ** 2\n",
        "  def perimeter(self) -> float: return 2 * math.pi * self.radius\n",
        "\n",
        "class Rectangle(Shape):\n",
        "  def __init__(self, length : float, width: float) -> float: self.length = length; self.width = width\n",
        "  def area(self) -> float: return self.length * self.width\n",
        "  def perimeter(self) -> float: return 2*self.length + 2*self.width\n",
        "\n",
        "# this file can be run in a new cell via : !pytest tests/shapes.py"
      ],
      "metadata": {
        "id": "u8hr58RXODdw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec8157a8-3f6e-4971-a5fa-47bc269f9a86"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting tests/shapes.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile tests/test_rectangle.py\n",
        "\n",
        "import pytest\n",
        "import math\n",
        "import shapes as shapes\n",
        "\n",
        "def test_area() -> float:\n",
        "  rectangle = shapes.Rectangle(10.0, 20.0)\n",
        "  print('\\nAsserting rectangle area: ', rectangle.area())\n",
        "  assert rectangle.area() == 10.0 * 20.0\n",
        "\n",
        "def test_perimeter() -> float:\n",
        "  rectangle = shapes.Rectangle(10.0, 20.0)\n",
        "  print('Asserting rectangle perimeter: ', rectangle.perimeter())\n",
        "  assert rectangle.perimeter() == 2*(10.0 + 20.0)\n"
      ],
      "metadata": {
        "id": "o88iDW6IujvB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8414a47-02f5-4bf9-a83a-2b07c0bd54a7"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing tests/test_rectangle.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest -s tests/test_rectangle.py"
      ],
      "metadata": {
        "id": "nHP7A5mewHfR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69f4e340-c7b4-4c3f-c743-ce6a6a123f9c"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.11, pytest-8.4.2, pluggy-1.6.0\n",
            "rootdir: /content\n",
            "plugins: langsmith-0.4.28, typeguard-4.4.4, anyio-4.10.0\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 2 items                                                              \u001b[0m\n",
            "\n",
            "tests/test_rectangle.py \n",
            "Asserting rectangle area:  200.0\n",
            "\u001b[32m.\u001b[0mAsserting rectangle perimeter:  60.0\n",
            "\u001b[32m.\u001b[0m\n",
            "\n",
            "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code, each test is creating a ```Rectangle``` object which is not efficient practice. We'll re-write the above code wherein a fixture would initialize a ```Rectangle``` object to be used in as many tests as we'd want to."
      ],
      "metadata": {
        "id": "FIHmotLo2bsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile tests/test_rectangle.py\n",
        "\n",
        "import pytest\n",
        "import math\n",
        "import shapes as shapes\n",
        "\n",
        "# Adding fixture\n",
        "@pytest.fixture\n",
        "def my_rectangle(): return shapes.Rectangle(10.0, 20.0)\n",
        "\n",
        "def test_area(my_rectangle) -> float:\n",
        "  print('\\nAsserting rectangle area: ', my_rectangle.area())\n",
        "  assert my_rectangle.area() == 10.0 * 20.0\n",
        "\n",
        "def test_perimeter(my_rectangle) -> float:\n",
        "  print('Asserting rectangle perimeter: ', my_rectangle.perimeter())\n",
        "  assert my_rectangle.perimeter() == 2*(10.0 + 20.0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsfBtGXM27lt",
        "outputId": "9b43ba9d-eddf-439f-df33-9f5059ff6c59"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting tests/test_rectangle.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest -s tests/test_rectangle.py"
      ],
      "metadata": {
        "id": "zNKYFo_x3VbB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0410d7e9-7ecf-4815-93ea-f09fe0751c4e"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.11, pytest-8.4.2, pluggy-1.6.0\n",
            "rootdir: /content\n",
            "plugins: langsmith-0.4.28, typeguard-4.4.4, anyio-4.10.0\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 2 items                                                              \u001b[0m\n",
            "\n",
            "tests/test_rectangle.py \n",
            "Asserting rectangle area:  200.0\n",
            "\u001b[32m.\u001b[0mAsserting rectangle perimeter:  60.0\n",
            "\u001b[32m.\u001b[0m\n",
            "\n",
            "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us update the definition of ```class Rectangle(Shape)``` in file ```shapes.py``` to the following:"
      ],
      "metadata": {
        "id": "WOiSv6Jp5wGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile tests/shapes.py\n",
        "\n",
        "import math\n",
        "\n",
        "class Shape:\n",
        "  def area(): pass\n",
        "  def perimeter(): pass\n",
        "\n",
        "class Circle(Shape):\n",
        "  def __init__(self, radius: float) -> float: self.radius = radius\n",
        "  def area(self) -> float: return math.pi * self.radius ** 2\n",
        "  def perimeter(self) -> float: return 2 * math.pi * self.radius\n",
        "\n",
        "class Rectangle(Shape):     # Updated class definition\n",
        "  def __init__(self, length : float, width: float) -> float:\n",
        "    self.length = length\n",
        "    self.width = width\n",
        "\n",
        "  def __eq__(self, other):  # Added new implementation of dunder fn __eq__\n",
        "    if not isinstance(other, Rectangle): return False\n",
        "    return self.length == other.length and self.width == other.width\n",
        "\n",
        "  def area(self) -> float: return self.length * self.width\n",
        "  def perimeter(self) -> float: return 2*self.length + 2*self.width"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odWPfVPV6YmR",
        "outputId": "3d255262-59d5-42fc-cb23-9779b37dde45"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting tests/shapes.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we add another fixture & test fn in file ```test_rectangle.py```:"
      ],
      "metadata": {
        "id": "tHajt_3v7II-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile tests/test_rectangle.py\n",
        "\n",
        "import pytest\n",
        "import shapes as shapes\n",
        "\n",
        "@pytest.fixture\n",
        "def my_rectangle(): return shapes.Rectangle(10.0, 20.0)\n",
        "\n",
        "@pytest.fixture\n",
        "def weird_rectangle(): return shapes.Rectangle(5, 6)         # Added another fixture\n",
        "\n",
        "def test_area(my_rectangle) -> float:\n",
        "  print('\\nAsserting rectangle area: ', my_rectangle.area())\n",
        "  assert my_rectangle.area() == 10.0 * 20.0\n",
        "\n",
        "def test_perimeter(my_rectangle) -> float:\n",
        "  print('Asserting rectangle perimeter: ', my_rectangle.perimeter())\n",
        "  assert my_rectangle.perimeter() == 2*(10.0 + 20.0)\n",
        "\n",
        "def test_not_equal(my_rectangle, weird_rectangle):           # Added another test\n",
        "  print('Asserting rectangles\\' equality: ', my_rectangle == weird_rectangle)  # '==' will look up '__eq__' fn in shapes.py\n",
        "  assert my_rectangle != weird_rectangle\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "semmeqTQ7ExQ",
        "outputId": "939db472-f4fd-422b-c119-81b21bdf62aa"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting tests/test_rectangle.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest -s tests/test_rectangle.py"
      ],
      "metadata": {
        "id": "1_BrloIV8Qay",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e37ab7c2-0ccc-4122-de25-ac97be3bc62d"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.11, pytest-8.4.2, pluggy-1.6.0\n",
            "rootdir: /content\n",
            "plugins: langsmith-0.4.28, typeguard-4.4.4, anyio-4.10.0\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 3 items                                                              \u001b[0m\n",
            "\n",
            "tests/test_rectangle.py \n",
            "Asserting rectangle area:  200.0\n",
            "\u001b[32m.\u001b[0mAsserting rectangle perimeter:  60.0\n",
            "\u001b[32m.\u001b[0mAsserting rectangles' equality:  False\n",
            "\u001b[32m.\u001b[0m\n",
            "\n",
            "\u001b[32m============================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making fixtures global within ```tests``` folder: The ```conftest.py``` file"
      ],
      "metadata": {
        "id": "8KY6mv1lHT6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if we had to use a ```Rectangle``` object in file ```test_circle.py``` for some reasons? Or any object in any file within the root folder ```tests```? We can't keep creating new objects for every use-case. So we will move all fixtures that create objects, to a separate file ```conftest.py```. Then, contents of ```conftest.py``` can be used by any file in folder ```tests```."
      ],
      "metadata": {
        "id": "2xAuCSNOGzSP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:**\n",
        "The ```conftest.py``` file in Python's ```pytest``` testing framework is a special configuration file that serves as a central location for defining shared fixtures, hooks, and plugins for test suite.\n",
        "\n",
        "Fixtures defined in ```conftest.py``` are **automatically discovered** by pytest and become available to all tests within the same directory and its subdirectories, **without needing explicit imports**. This promotes code reusability for setup and teardown operations, such as creating database connections, setting up test data, or initializing mock objects.\n",
        "\n",
        "You can have multiple ```conftest.py``` files in different directories within your test suite. Fixtures defined in a parent directory's ```conftest.py``` are available to all tests in that directory and its subdirectories, while a ```conftest.py``` in a subdirectory can define its own fixtures or override those from parent directories, allowing for fine-grained control over fixture availability."
      ],
      "metadata": {
        "id": "l1ovS0zFKlR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile tests/conftest.py\n",
        "\n",
        "import pytest\n",
        "import shapes as shapes\n",
        "\n",
        "@pytest.fixture\n",
        "def my_rectangle(): return shapes.Rectangle(10.0, 20.0)\n",
        "\n",
        "@pytest.fixture\n",
        "def weird_rectangle(): return shapes.Rectangle(5, 6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S003S3lKImOp",
        "outputId": "7c4c5022-7b86-44bf-e6bd-ea533b7befb2"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing tests/conftest.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's overwrite the file ```test_rectangle.py``` :"
      ],
      "metadata": {
        "id": "dhnbr38DJv1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile tests/test_rectangle.py\n",
        "\n",
        "import pytest\n",
        "\n",
        "# Fixture present here earlier moved to conftest.py file\n",
        "\n",
        "def test_area(my_rectangle) -> float:\n",
        "  print('\\nAsserting rectangle area: ', my_rectangle.area())\n",
        "  assert my_rectangle.area() == 10.0 * 20.0\n",
        "\n",
        "def test_perimeter(my_rectangle) -> float:\n",
        "  print('Asserting rectangle perimeter: ', my_rectangle.perimeter())\n",
        "  assert my_rectangle.perimeter() == 2*(10.0 + 20.0)\n",
        "\n",
        "def test_not_equal(my_rectangle, weird_rectangle):           # Added another test\n",
        "  print('Asserting rectangles\\' equality: ', my_rectangle == weird_rectangle)  # '==' will look up '__eq__' fn in shapes.py\n",
        "  assert my_rectangle != weird_rectangle\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fo0kWs59J2rj",
        "outputId": "a8bf75fc-099a-457c-9c2b-d0380cb7d753"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting tests/test_rectangle.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the above file:"
      ],
      "metadata": {
        "id": "zbPXAToftZts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest -s tests/test_rectangle.py"
      ],
      "metadata": {
        "id": "f7h58SrhKZwN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4f6d90e-9efc-4968-f2bf-a9e1390a4cbd"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.11, pytest-8.4.2, pluggy-1.6.0\n",
            "rootdir: /content\n",
            "plugins: langsmith-0.4.28, typeguard-4.4.4, anyio-4.10.0\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 3 items                                                              \u001b[0m\n",
            "\n",
            "tests/test_rectangle.py \n",
            "Asserting rectangle area:  200.0\n",
            "\u001b[32m.\u001b[0mAsserting rectangle perimeter:  60.0\n",
            "\u001b[32m.\u001b[0mAsserting rectangles' equality:  False\n",
            "\u001b[32m.\u001b[0m\n",
            "\n",
            "\u001b[32m============================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a ```conftest``` file, let us alter the ```test_circle.py``` file:"
      ],
      "metadata": {
        "id": "uFZRfwHjtryj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile tests/test_circle.py\n",
        "\n",
        "import pytest\n",
        "import math\n",
        "import shapes as shapes\n",
        "\n",
        "class TestCircle:\n",
        "  def setup_method(self, method):\n",
        "    print(f\"\\n\\nSetting up {method}.\")\n",
        "    self.circle = shapes.Circle(10)\n",
        "\n",
        "  def teardown_method(self, method):\n",
        "    print(f\"Tearing down {method}\")\n",
        "    del self.circle\n",
        "\n",
        "  def test_area(self) -> float:\n",
        "    print(\"Asserting area of circle :\", math.pi * self.circle.radius ** 2)\n",
        "    assert self.circle.area() == math.pi * self.circle.radius ** 2\n",
        "\n",
        "  def test_perimeter(self) -> float:\n",
        "    print(\"Asserting perimeter of circle: \", math.pi * self.circle.radius)\n",
        "    assert self.circle.perimeter() == 2 * math.pi * self.circle.radius\n",
        "\n",
        "  def test_not_same_area_rectangle(self, my_rectangle):     # NEWLY ADDED IN THIS FILE\n",
        "    print('Asserting circle area ', self.circle.area(), '& my_rect. area ', my_rectangle.area(),' : ',self.circle.area() != my_rectangle.area())\n",
        "    assert self.circle.area() != my_rectangle.area()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJN3JIWWt8IR",
        "outputId": "b48191cd-632b-4f91-c015-11b760d89df4"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting tests/test_circle.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest -s tests/test_circle.py"
      ],
      "metadata": {
        "id": "SeGuMhCivMnX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c68ed77c-0f76-41b9-ec7d-299f21ca85e2"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.11, pytest-8.4.2, pluggy-1.6.0\n",
            "rootdir: /content\n",
            "plugins: langsmith-0.4.28, typeguard-4.4.4, anyio-4.10.0\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 3 items                                                              \u001b[0m\n",
            "\n",
            "tests/test_circle.py \n",
            "\n",
            "Setting up <bound method TestCircle.test_area of <test_circle.TestCircle object at 0x1342aa25d7f0>>.\n",
            "Asserting area of circle : 314.1592653589793\n",
            "\u001b[32m.\u001b[0mTearing down <bound method TestCircle.test_area of <test_circle.TestCircle object at 0x1342aa25d7f0>>\n",
            "\n",
            "\n",
            "Setting up <bound method TestCircle.test_perimeter of <test_circle.TestCircle object at 0x1342aa27ed50>>.\n",
            "Asserting perimeter of circle:  31.41592653589793\n",
            "\u001b[32m.\u001b[0mTearing down <bound method TestCircle.test_perimeter of <test_circle.TestCircle object at 0x1342aa27ed50>>\n",
            "\n",
            "\n",
            "Setting up <bound method TestCircle.test_not_same_area_rectangle of <test_circle.TestCircle object at 0x1342aa27ce30>>.\n",
            "Asserting circle area  314.1592653589793 & my_rect. area  200.0  :  True\n",
            "\u001b[32m.\u001b[0mTearing down <bound method TestCircle.test_not_same_area_rectangle of <test_circle.TestCircle object at 0x1342aa27ce30>>\n",
            "\n",
            "\n",
            "\u001b[32m============================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Markers: ```@pytest.mark```"
      ],
      "metadata": {
        "id": "kMEvPhJACK8k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Markers are **labels** used to categorize tests using ```@pytest.mark``` so that only selected tests may be run."
      ],
      "metadata": {
        "id": "mM532maYQ059"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile tests/my_functions.py\n",
        "\n",
        "# import pytest\n",
        "\n",
        "def add(num1, num2): return num1 + num2   # Can concat strings too\n",
        "def divide(num1, num2):\n",
        "  if num2 == 0: raise ValueError\n",
        "  return num1 / num2\n"
      ],
      "metadata": {
        "id": "sANcMXrKCNzR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0085f5c9-9f5a-4cdd-bf3b-66db5120fd29"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing tests/my_functions.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile tests/test_my_functions.py\n",
        "\n",
        "import my_functions as my_functions\n",
        "import pytest\n",
        "import time\n",
        "\n",
        "def test_add():\n",
        "  print(\"\\nTesting addition ... OK!\")\n",
        "  result = my_functions.add(1, 4)\n",
        "  assert result == 5\n",
        "\n",
        "def test_add_strings():\n",
        "  print(\"Testing str concat :\", my_functions.add(\"I like \", \"burgers\"), \"... OK!\")\n",
        "  result = my_functions.add(\"I like \", \"burgers\")\n",
        "  assert result == \"I like burgers\"\n",
        "\n",
        "def test_divide():\n",
        "  print(\"Testing division ... OK!\")\n",
        "  result = my_functions.divide(10, 5)\n",
        "  assert result == 2\n",
        "\n",
        "def test_divide_by_zero():\n",
        "  print(\"Testing division by zero ... OK!\")\n",
        "  with pytest.raises(ValueError):\n",
        "    my_functions.divide(10, 0)\n",
        "\n",
        "def test_very_slow():\n",
        "  time.sleep(5)             # from 'time' package\n",
        "  print(\"Testing slowness ... OK!\")\n",
        "  result = my_functions.add(1, 4)\n",
        "  assert result == 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-NGi5p7hqcy",
        "outputId": "fade7739-e26c-427e-c05e-7399c096588c"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing tests/test_my_functions.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest -s tests/test_my_functions.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMkX9ZVxzMBU",
        "outputId": "9782919f-8729-4a4f-a949-44a5ef39ebd9"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.11, pytest-8.4.2, pluggy-1.6.0\n",
            "rootdir: /content\n",
            "plugins: langsmith-0.4.28, typeguard-4.4.4, anyio-4.10.0\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 5 items                                                              \u001b[0m\n",
            "\n",
            "tests/test_my_functions.py \n",
            "Testing addition ... OK!\n",
            "\u001b[32m.\u001b[0mTesting str concat : I like burgers ... OK!\n",
            "\u001b[32m.\u001b[0mTesting division ... OK!\n",
            "\u001b[32m.\u001b[0mTesting division by zero ... OK!\n",
            "\u001b[32m.\u001b[0mTesting slowness ... OK!\n",
            "\u001b[32m.\u001b[0m\n",
            "\n",
            "\u001b[32m============================== \u001b[32m\u001b[1m5 passed\u001b[0m\u001b[32m in 5.02s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the above file has demonstrated a slowly running function, let us mark it as ```pytest.mark.slow``` so that we can run only the ```slow``` test (& not the rest 4 tests):"
      ],
      "metadata": {
        "id": "xeCNZcL01Hvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile tests/test_my_functions.py\n",
        "\n",
        "import my_functions as my_functions\n",
        "import pytest\n",
        "import time\n",
        "\n",
        "def test_add():\n",
        "  print(\"\\nTesting addition ... OK!\")\n",
        "  result = my_functions.add(1, 4)\n",
        "  assert result == 5\n",
        "\n",
        "def test_add_strings():\n",
        "  print(\"Testing str concat :\", my_functions.add(\"I like \", \"burgers\"), \"... OK!\")\n",
        "  result = my_functions.add(\"I like \", \"burgers\")\n",
        "  assert result == \"I like burgers\"\n",
        "\n",
        "def test_divide():\n",
        "  print(\"Testing division ... OK!\")\n",
        "  result = my_functions.divide(10, 5)\n",
        "  assert result == 2\n",
        "\n",
        "def test_divide_by_zero():\n",
        "  print(\"Testing division by zero ... OK!\")\n",
        "  with pytest.raises(ValueError):\n",
        "    my_functions.divide(10, 0)\n",
        "\n",
        "@pytest.mark.slow                      # marked as 'slow'\n",
        "def test_very_slow():\n",
        "  time.sleep(3)\n",
        "  print(\"\\n\\nTesting slowness ... OK!\")  # will get printed after 3 seconds\n",
        "  result = my_functions.add(1, 4)\n",
        "  assert result == 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2IRd78O1Z49",
        "outputId": "d35e283f-dfe6-43f7-b4ca-aaf0d2149707"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting tests/test_my_functions.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After marking, we need to register the ```slow``` marker in ```pytest.ini``` file:"
      ],
      "metadata": {
        "id": "GKahkDQG4tol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Registering markers in a .ini file.\n",
        "# This file should be present in root directory.\n",
        "# Enables one to run only one type of test at console (e.g. only smoke, or only sanity, etc.)\n",
        "\n",
        "%%writefile tests/pytest.ini\n",
        "\n",
        "# pytest.ini\n",
        "[pytest]\n",
        "markers =\n",
        "    sanity: marks tests as sanity tests\n",
        "    smoke: marks tests as smoke tests\n",
        "    slow: marks tests as slow tests           # NEWLY ADDED"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdmTSiQE2SPM",
        "outputId": "042043eb-54bb-4326-c0ad-73a01e52b7dc"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing tests/pytest.ini\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now we run only the 'slow' marked test:"
      ],
      "metadata": {
        "id": "6gj3ryP62un8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest -s -m slow tests/test_my_functions.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mO_CYyu1y0f",
        "outputId": "240fa4df-9f84-4388-b501-93f309d5bfc9"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.11, pytest-8.4.2, pluggy-1.6.0\n",
            "rootdir: /content/tests\n",
            "configfile: pytest.ini\n",
            "plugins: langsmith-0.4.28, typeguard-4.4.4, anyio-4.10.0\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 5 items / 4 deselected / 1 selected                                  \u001b[0m\n",
            "\n",
            "tests/test_my_functions.py \n",
            "\n",
            "Testing slowness ... OK!\n",
            "\u001b[32m.\u001b[0m\n",
            "\n",
            "\u001b[32m======================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m4 deselected\u001b[0m\u001b[32m in 3.01s\u001b[0m\u001b[32m ========================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's add a marker ```skip``` now:"
      ],
      "metadata": {
        "id": "qv2AldVXhq6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile tests/test_my_functions.py\n",
        "\n",
        "import my_functions as my_functions\n",
        "import pytest\n",
        "import time\n",
        "\n",
        "def test_add():\n",
        "  print(\"\\nTesting addition ... OK!\")\n",
        "  result = my_functions.add(1, 4)\n",
        "  assert result == 5\n",
        "\n",
        "def test_add_strings():\n",
        "  print(\"Testing str concat :\", my_functions.add(\"I like \", \"burgers\"), \"... OK!\")\n",
        "  result = my_functions.add(\"I like \", \"burgers\")\n",
        "  assert result == \"I like burgers\"\n",
        "\n",
        "def test_divide():\n",
        "  print(\"Testing division ... OK!\")\n",
        "  result = my_functions.divide(10, 5)\n",
        "  assert result == 2\n",
        "\n",
        "def test_divide_by_zero():\n",
        "  print(\"Testing division by zero ... OK!\")\n",
        "  with pytest.raises(ValueError):\n",
        "    my_functions.divide(10, 0)\n",
        "\n",
        "@pytest.mark.slow\n",
        "def test_very_slow():\n",
        "  time.sleep(3)\n",
        "  print(\"\\n\\nTesting slowness ... OK!\")\n",
        "  result = my_functions.add(1, 4)\n",
        "  assert result == 5\n",
        "\n",
        "@pytest.mark.skip(reason = \"This feature is broken\")  # Added this\n",
        "def test_adding():\n",
        "  print(\"Testing skipping ... OK!\") # won't print since 'skipping' this fn\n",
        "  assert my_functions.add(1,2) == 3\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKhmwzO5hyWB",
        "outputId": "8c38f5e5-e2d8-4a7f-a6e4-183dd3dc87a3"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting tests/test_my_functions.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, register the ```skip``` marker in ```pytest.ini``` file:"
      ],
      "metadata": {
        "id": "GxwL4Fd0ij6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Registering markers in a .ini file.\n",
        "# This file should be present in root directory.\n",
        "# Enables one to run only one type of test at console (e.g. only smoke, or only sanity, etc.)\n",
        "\n",
        "%%writefile tests/pytest.ini\n",
        "\n",
        "# pytest.ini\n",
        "[pytest]\n",
        "markers =\n",
        "    sanity: marks tests as sanity tests\n",
        "    smoke: marks tests as smoke tests\n",
        "    slow: marks tests as slow tests\n",
        "    skip: marks tests to skip              # NEWLY ADDED"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsxvlxSSiiGx",
        "outputId": "be459406-fba4-4182-e38d-d1de5d9cf1ee"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting tests/pytest.ini\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest -s -m skip tests/test_my_functions.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoaK_GJoizzM",
        "outputId": "2189b375-9527-41fe-f773-aedfd1094da0"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.11, pytest-8.4.2, pluggy-1.6.0\n",
            "rootdir: /content/tests\n",
            "configfile: pytest.ini\n",
            "plugins: langsmith-0.4.28, typeguard-4.4.4, anyio-4.10.0\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 6 items / 5 deselected / 1 selected                                  \u001b[0m\n",
            "\n",
            "tests/test_my_functions.py \u001b[33ms\u001b[0m\n",
            "\n",
            "\u001b[33m======================= \u001b[33m\u001b[1m1 skipped\u001b[0m, \u001b[33m\u001b[1m5 deselected\u001b[0m\u001b[33m in 0.01s\u001b[0m\u001b[33m =======================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding another marker ```xfail``` :"
      ],
      "metadata": {
        "id": "3TWhku9bkalm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile tests/test_my_functions.py\n",
        "\n",
        "import my_functions as my_functions\n",
        "import pytest\n",
        "import time\n",
        "\n",
        "def test_add():\n",
        "  print(\"\\nTesting addition ... OK!\")\n",
        "  result = my_functions.add(1, 4)\n",
        "  assert result == 5\n",
        "\n",
        "def test_add_strings():\n",
        "  print(\"Testing str concat :\", my_functions.add(\"I like \", \"burgers\"), \"... OK!\")\n",
        "  result = my_functions.add(\"I like \", \"burgers\")\n",
        "  assert result == \"I like burgers\"\n",
        "\n",
        "def test_divide():\n",
        "  print(\"Testing division ... OK!\")\n",
        "  result = my_functions.divide(10, 5)\n",
        "  assert result == 2\n",
        "\n",
        "def test_divide_by_zero():\n",
        "  print(\"Testing division by zero ... OK!\")\n",
        "  with pytest.raises(ValueError):\n",
        "    my_functions.divide(10, 0)\n",
        "\n",
        "@pytest.mark.slow\n",
        "def test_very_slow():\n",
        "  time.sleep(3)\n",
        "  print(\"\\n\\nTesting slowness ... OK!\")\n",
        "  result = my_functions.add(1, 4)\n",
        "  assert result == 5\n",
        "\n",
        "@pytest.mark.skip(reason = \"This feature is broken\")\n",
        "def test_adding():\n",
        "  print(\"Testing skipping ... OK!\")\n",
        "  assert my_functions.add(1,2) == 3\n",
        "\n",
        "@pytest.mark.xfail(reason = 'We know can\\'t divide by zero')  # Added this\n",
        "def test_divide_zero_broken():\n",
        "  my_functions.divide(4, 0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrmxQBgukdd6",
        "outputId": "85fb12e2-1796-4e6f-9670-ad92c8373971"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting tests/test_my_functions.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest tests/test_my_functions.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slCOKjerk9BE",
        "outputId": "e94c14ef-60e1-4b69-f9ca-9288c7064c30"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.11, pytest-8.4.2, pluggy-1.6.0\n",
            "rootdir: /content/tests\n",
            "configfile: pytest.ini\n",
            "plugins: langsmith-0.4.28, typeguard-4.4.4, anyio-4.10.0\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 7 items                                                              \u001b[0m\n",
            "\n",
            "tests/test_my_functions.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[33ms\u001b[0m\u001b[33mx\u001b[0m\u001b[32m                                       [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m=================== \u001b[32m\u001b[1m5 passed\u001b[0m, \u001b[33m1 skipped\u001b[0m, \u001b[33m1 xfailed\u001b[0m\u001b[32m in 3.09s\u001b[0m\u001b[32m ====================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parametrization: ```pytest.mark.parametrize```"
      ],
      "metadata": {
        "id": "zYA9F-rlmWWQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us update the ```shapes.py``` file with new class ```square```:"
      ],
      "metadata": {
        "id": "sgR9WkIFmoon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile tests/shapes.py\n",
        "\n",
        "import math\n",
        "\n",
        "class Shape:\n",
        "  def area(): pass\n",
        "  def perimeter(): pass\n",
        "\n",
        "class Circle(Shape):\n",
        "  def __init__(self, radius: float) -> float: self.radius = radius\n",
        "  def area(self) -> float: return math.pi * self.radius ** 2\n",
        "  def perimeter(self) -> float: return 2 * math.pi * self.radius\n",
        "\n",
        "class Rectangle(Shape):\n",
        "  def __init__(self, length : float, width: float) -> float:\n",
        "    self.length = length\n",
        "    self.width = width\n",
        "\n",
        "  def __eq__(self, other):\n",
        "    if not isinstance(other, Rectangle): return False\n",
        "    return self.length == other.length and self.width == other.width\n",
        "\n",
        "  def area(self) -> float: return self.length * self.width\n",
        "  def perimeter(self) -> float: return 2*self.length + 2*self.width\n",
        "\n",
        "class Square(Rectangle):  # Added this new class which inherits from 'Rectangle'\n",
        "  def __init__(self, side_length: float) -> float:\n",
        "    super().__init__(side_length, side_length)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E54fkleamcLy",
        "outputId": "e677d65e-d7f1-40f0-dede-0d197f2f0d72"
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting tests/shapes.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we create a new test file ```test_square.py```. We want to input multiple values into the test function so we'll add parametrization fixture as done ahead:"
      ],
      "metadata": {
        "id": "cMyLxwBVn23j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile tests/test_square.py\n",
        "\n",
        "import pytest\n",
        "import shapes as shapes\n",
        "\n",
        "# ADDING PARAMETRIZATION MARKER\n",
        "@pytest.mark.parametrize(\"side_length, expected_area\", [(5, 25), (4, 16), (9, 81)])\n",
        "def test_multiple_square_areas(side_length, expected_area):\n",
        "  print(\"\\nTesting square side: \", side_length, \", area: \", shapes.Square(side_length).area())\n",
        "  assert shapes.Square(side_length).area() == expected_area"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvD2paLGn7Pq",
        "outputId": "753a5484-07ea-44eb-925f-951408024e75"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing tests/test_square.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest -s tests/test_square.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IR1vmaPvEvq",
        "outputId": "20f98ace-0520-41bc-803a-e7769170a5fc"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.11, pytest-8.4.2, pluggy-1.6.0\n",
            "rootdir: /content/tests\n",
            "configfile: pytest.ini\n",
            "plugins: langsmith-0.4.28, typeguard-4.4.4, anyio-4.10.0\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 3 items                                                              \u001b[0m\n",
            "\n",
            "tests/test_square.py \n",
            "Testing square side:  5 , area:  25\n",
            "\u001b[32m.\u001b[0m\n",
            "Testing square side:  4 , area:  16\n",
            "\u001b[32m.\u001b[0m\n",
            "Testing square side:  9 , area:  81\n",
            "\u001b[32m.\u001b[0m\n",
            "\n",
            "\u001b[32m============================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we add another parametrization marker for perimeter this time:"
      ],
      "metadata": {
        "id": "XO1eGqhkwll8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile tests/test_square.py\n",
        "\n",
        "import pytest\n",
        "import shapes as shapes\n",
        "\n",
        "\n",
        "@pytest.mark.parametrize(\"side_length, expected_area\", [(5, 25), (4, 16), (9, 81)])\n",
        "def test_multiple_square_areas(side_length, expected_area):\n",
        "  print(\"Testing square side: \", side_length, \", area: \", shapes.Square(side_length).area())\n",
        "  assert shapes.Square(side_length).area() == expected_area\n",
        "\n",
        "# ADDING ANOTHER PARAMETRIZATION MARKER\n",
        "@pytest.mark.parametrize(\"side_length, expected_perimeter\", [(3, 12), (4, 16), (5, 20)])\n",
        "def test_multiple_perimeters(side_length, expected_perimeter):\n",
        "  print(\"Testing square side: \", side_length, \", perimeter: \", shapes.Square(side_length).perimeter())\n",
        "  assert shapes.Square(side_length).perimeter() == expected_perimeter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJw0kKDmwqt1",
        "outputId": "3c5c99c0-a4e0-406f-acd5-f67b7ca01a1c"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting tests/test_square.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest -s tests/test_square.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eB4zzJuSxACo",
        "outputId": "b0ec6e69-6c5a-43c4-ccc9-dcd4e704fd4a"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.11, pytest-8.4.2, pluggy-1.6.0\n",
            "rootdir: /content/tests\n",
            "configfile: pytest.ini\n",
            "plugins: langsmith-0.4.28, typeguard-4.4.4, anyio-4.10.0\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 6 items                                                              \u001b[0m\n",
            "\n",
            "tests/test_square.py Testing square side:  5 , area:  25\n",
            "\u001b[32m.\u001b[0mTesting square side:  4 , area:  16\n",
            "\u001b[32m.\u001b[0mTesting square side:  9 , area:  81\n",
            "\u001b[32m.\u001b[0mTesting square side:  3 , perimeter:  12\n",
            "\u001b[32m.\u001b[0mTesting square side:  4 , perimeter:  16\n",
            "\u001b[32m.\u001b[0mTesting square side:  5 , perimeter:  20\n",
            "\u001b[32m.\u001b[0m\n",
            "\n",
            "\u001b[32m============================== \u001b[32m\u001b[1m6 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mocking"
      ],
      "metadata": {
        "id": "ukoxTjvCyIWh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mocking can be thought of as creating a \"dummy\" version of a component that mimics real behavior. A mock object simulates the behavior of a real object. It is often used in unit tests to isolate components and test their behavior without executing dependent code. It is especially helpful when you need to test how functions or classes interact with external components like databases or external APIs but executing those real-time interaction would raise costs.\n",
        "\n",
        "If we already know the behavior of that execution, we can simulate it through mocks. As an example, suppose we want to pull a record from a live database. There could be network issues which might fail the test. So since we already know what pulling a record from DB would be like, we can mock the process."
      ],
      "metadata": {
        "id": "J0zmGkm29Xi_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a new file ```service.py```:"
      ],
      "metadata": {
        "id": "-fCiO4dhaGrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile tests/service.py\n",
        "\n",
        "database = {1: \"Alice\", 2: \"Bob\", 3: \"Charlie\"}\n",
        "\n",
        "def get_user_from_db(user_id): return database.get(user_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEuzy7NhaLIB",
        "outputId": "69acce48-150b-461e-96b1-bb41f481f64f"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing tests/service.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile tests/test_service.py\n",
        "\n",
        "import pytest\n",
        "import service as service\n",
        "import unittest.mock as mock\n",
        "\n",
        "@mock.patch('service.get_user_from_db')   # 'patch' takes path to the method/class/object etc.\n",
        "def test_get_user_from_db(mock_get_user_from_db):  # the arg can be named anything\n",
        "  mock_get_user_from_db.return_value = \"Mocked Alice\"\n",
        "  user_name = service.get_user_from_db(1)\n",
        "  print(\"User name: \", user_name)\n",
        "  # print('Asserting Alice: ', mock_get_user_from_db.return_value)\n",
        "  assert user_name == \"Mocked Alice\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkhkizPealxb",
        "outputId": "07bd5805-a70f-49de-a7c0-1dac3567b9a2"
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing tests/test_service.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest -s tests/test_service.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nL0hyXQ7yJzF",
        "outputId": "708e6ad3-1197-4ba5-e1da-4cf77a0f9db4"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.11, pytest-8.4.2, pluggy-1.6.0\n",
            "rootdir: /content/tests\n",
            "configfile: pytest.ini\n",
            "plugins: langsmith-0.4.28, typeguard-4.4.4, anyio-4.10.0\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 1 item                                                               \u001b[0m\n",
            "\n",
            "tests/test_service.py User name:  Mocked Alice\n",
            "\u001b[32m.\u001b[0m\n",
            "\n",
            "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we modify the file ```service.py``` to add ```requests``` package for making an API call in file ```test_service.py.```"
      ],
      "metadata": {
        "id": "7K_RXHzSecJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile tests/service.py\n",
        "\n",
        "import requests\n",
        "\n",
        "database = {1: \"Alice\", 2: \"Bob\", 3: \"Charlie\"}\n",
        "\n",
        "def get_user_from_db(user_id): return database.get(user_id)\n",
        "\n",
        "def get_users():    # pulling a dummy data available online in JSON format at a dummy website\n",
        "  response = requests.get(\"https://jsonplaceholder.typicode.com/users\")\n",
        "  if response.status_code == 200:\n",
        "    return response.json()\n",
        "  raise requests.HTTPError\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bR6S3tM9ex6D",
        "outputId": "7c405ec0-9313-48bd-ca42-67e69a0e6d19"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting tests/service.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# UPDATING THIS FILE\n",
        "\n",
        "%%writefile tests/test_service.py\n",
        "\n",
        "import pytest\n",
        "import service as service\n",
        "import unittest.mock as mock\n",
        "\n",
        "@mock.patch('service.get_user_from_db')   # 'patch' takes path to the method/class/object etc.\n",
        "def test_get_user_from_db(mock_get_user_from_db):  # the arg can be named anything\n",
        "  mock_get_user_from_db.return_value = \"Mocked Alice\"\n",
        "  user_name = service.get_user_from_db(1)\n",
        "  print(\"\\nUser name: \", user_name)\n",
        "  assert user_name == \"Mocked Alice\"\n",
        "  print('\\nAsserting Alice: ', mock_get_user_from_db.return_value)\n",
        "\n",
        "@mock.patch(\"requests.get\")\n",
        "def test_get_users(mock_get):   # 'mock_get' will override 'requests.get'\n",
        "  mock_response = mock.Mock()\n",
        "  mock_response.status_code = 200\n",
        "  mock_response.json.return_value = {\"id\": 1, \"name\": \"John Doe\"}\n",
        "  mock_get.return_value = mock_response\n",
        "  data = service.get_users()\n",
        "  assert data == {\"id\": 1, \"name\": \"John Doe\"}\n",
        "  print(\"Mocked requests data: \", data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypZ-DwsZf3gq",
        "outputId": "bb547659-4deb-4be6-f9bd-affd346366d6"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting tests/test_service.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest -s tests/test_service.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2ocqGeZiIzA",
        "outputId": "8aa37fa1-0bdc-4917-fcdf-e6a988d8ab46"
      },
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.11, pytest-8.4.2, pluggy-1.6.0\n",
            "rootdir: /content/tests\n",
            "configfile: pytest.ini\n",
            "plugins: langsmith-0.4.28, typeguard-4.4.4, anyio-4.10.0\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 2 items                                                              \u001b[0m\n",
            "\n",
            "tests/test_service.py \n",
            "User name:  Mocked Alice\n",
            "\n",
            "Asserting Alice:  Mocked Alice\n",
            "\u001b[32m.\u001b[0mMocked requests data:  {'id': 1, 'name': 'John Doe'}\n",
            "\u001b[32m.\u001b[0m\n",
            "\n",
            "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's add a test for HTTP error :"
      ],
      "metadata": {
        "id": "dLq5yKTVjvqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UPDATING THIS FILE\n",
        "\n",
        "%%writefile tests/test_service.py\n",
        "\n",
        "import requests\n",
        "import pytest\n",
        "import service as service\n",
        "import unittest.mock as mock\n",
        "\n",
        "@mock.patch('service.get_user_from_db')\n",
        "def test_get_user_from_db(mock_get_user_from_db):\n",
        "  mock_get_user_from_db.return_value = \"Mocked Alice\"\n",
        "  user_name = service.get_user_from_db(1)\n",
        "  print(\"\\nUser name: \", user_name)\n",
        "  assert user_name == \"Mocked Alice\"\n",
        "  print('\\nAsserting Alice: ', mock_get_user_from_db.return_value)\n",
        "\n",
        "@mock.patch(\"requests.get\")\n",
        "def test_get_users(mock_get):\n",
        "  mock_response = mock.Mock()\n",
        "  mock_response.status_code = 200\n",
        "  mock_response.json.return_value = {\"id\": 1, \"name\": \"John Doe\"}\n",
        "  mock_get.return_value = mock_response\n",
        "  data = service.get_users()\n",
        "  assert data == {\"id\": 1, \"name\": \"John Doe\"}\n",
        "  print(\"Mocked requests data: \", data)\n",
        "\n",
        "@mock.patch(\"requests.get\")\n",
        "def test_get_users_error(mock_get):     # fn to test HTTP Error\n",
        "  mock_response = mock.Mock()\n",
        "  mock_response.status_code = 400\n",
        "  mock_get.return_value = mock_response\n",
        "\n",
        "  with pytest.raises(requests.HTTPError):\n",
        "    service.get_users()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YVg-BM2j0g9",
        "outputId": "8e01ea49-339f-4e7f-8659-5b169505fcea"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting tests/test_service.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest -s tests/test_service.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_v4vj5mCkZbx",
        "outputId": "4c562f1c-61a4-4a29-ac0d-1399f0bf43a4"
      },
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.11, pytest-8.4.2, pluggy-1.6.0\n",
            "rootdir: /content/tests\n",
            "configfile: pytest.ini\n",
            "plugins: langsmith-0.4.28, typeguard-4.4.4, anyio-4.10.0\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 3 items                                                              \u001b[0m\n",
            "\n",
            "tests/test_service.py \n",
            "User name:  Mocked Alice\n",
            "\n",
            "Asserting Alice:  Mocked Alice\n",
            "\u001b[32m.\u001b[0mMocked requests data:  {'id': 1, 'name': 'John Doe'}\n",
            "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\n",
            "\n",
            "\u001b[32m============================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing Powered By AI (GPT tools)"
      ],
      "metadata": {
        "id": "VhQ8Tsnembs6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll create a skeleton of many classes and methods within them and then copy-paste the definitions in GPT tool & ask it to generate test cases."
      ],
      "metadata": {
        "id": "uz8kIBtQvs0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile tests/school.py\n",
        "\n",
        "class TooManyStudents(Exception):  # this class created after class 'Classroom'\n",
        "  pass\n",
        "\n",
        "class Classroom():\n",
        "  def __init__(self, teacher, students, course_title):\n",
        "    self.teacher = teacher\n",
        "    self.students = students\n",
        "    self.course_title = course_title\n",
        "\n",
        "  def add_student(self, student):\n",
        "    if len(self.students) <= 10: self.students.append(student)\n",
        "    else: raise TooManyStudents\n",
        "\n",
        "  def remove_student(self, name):\n",
        "    for student in self.students:\n",
        "      if student.name == name:\n",
        "        self.students.remove(student)\n",
        "        break\n",
        "\n",
        "  def change_teacher(self, new_teacher): self.teacher = new_teacher\n",
        "\n",
        "class Person:\n",
        "  def __init__(self, name): self.name = name\n",
        "\n",
        "class Teacher(Person): pass\n",
        "\n",
        "class Student(Person): pass\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ms4wy1XRmhLE",
        "outputId": "d1579858-80b2-4c75-9ba5-de4f4b4b847d"
      },
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing tests/school.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Open a GPT tool. Enter the prompt:\n",
        "\n",
        "```\n",
        "Using the pytest package of python, use the functions like parametrization, raises, mark, etc. wherever necessary to test the following code themed on Harry Potter world:\n",
        "```\n",
        "\n",
        "Copy paste the prev cell's code in front of the above prompt. Hit Enter. Copy the code resulting in the GPT and paste it in the next cell as done here ahead:"
      ],
      "metadata": {
        "id": "9xu6pm6vr7eZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile tests/test_classroom.py\n",
        "\n",
        "\n",
        "from school import Classroom, Teacher, Student, TooManyStudents   # make sure to add this import\n",
        "\n",
        "# ENTIRE CODE BELOW IS GENERATED BY GPT:\n",
        "\n",
        "import pytest\n",
        "\n",
        "# Helper to create test students quickly\n",
        "def make_students(n, name_prefix=\"Student\"):\n",
        "    return [Student(f\"{name_prefix}{i}\") for i in range(n)]\n",
        "\n",
        "# Fixtures\n",
        "@pytest.fixture\n",
        "def sample_teacher(): return Teacher(\"Albus Dumbledore\")\n",
        "\n",
        "@pytest.fixture\n",
        "def gof_course(sample_teacher):\n",
        "    # small classroom used in multiple tests\n",
        "    return Classroom(teacher=sample_teacher, students=make_students(3), course_title=\"G.O.F.\")\n",
        "\n",
        "\n",
        "# Tests ----------------------------------------------------------\n",
        "\n",
        "@pytest.mark.parametrize(\"initial_count\", [0, 5, 10])\n",
        "def test_add_student_within_limit(initial_count):\n",
        "    \"\"\"\n",
        "    Logic: When classroom has <= 10 students, add_student should append the new student.\n",
        "    We parametrize across edge-cases including exactly 10 (allowed) and lower counts.\n",
        "    \"\"\"\n",
        "    teacher = Teacher(\"Minerva McGonagall\")\n",
        "    students = make_students(initial_count)\n",
        "    cls = Classroom(teacher, students, \"Transfiguration\")\n",
        "    new = Student(\"Hermione Granger\")\n",
        "\n",
        "    cls.add_student(new)\n",
        "\n",
        "    assert cls.students[-1] is new  # appended object is exactly the object we passed\n",
        "    assert len(cls.students) == initial_count + 1\n",
        "\n",
        "\n",
        "@pytest.mark.parametrize(\"initial_count\", [11, 12, 20])\n",
        "def test_add_student_raises_toomanystudents_when_over_limit(initial_count):\n",
        "    \"\"\"\n",
        "    Logic: The implementation raises TooManyStudents only when len(self.students) > 10.\n",
        "    So for initial_count > 10, add_student must raise.\n",
        "    \"\"\"\n",
        "    teacher = Teacher(\"Severus Snape\")\n",
        "    students = make_students(initial_count)\n",
        "    cls = Classroom(teacher, students, \"Potions\")\n",
        "    new = Student(\"Neville Longbottom\")\n",
        "\n",
        "    with pytest.raises(TooManyStudents):\n",
        "        cls.add_student(new)\n",
        "\n",
        "\n",
        "def test_add_student_at_exact_threshold_allows_11th_student():\n",
        "    \"\"\"\n",
        "    Logic: Because condition is `if len(self.students) <= 10: append`,\n",
        "    starting with 10 allows adding one more (to become 11).\n",
        "    \"\"\"\n",
        "    teacher = Teacher(\"Remus Lupin\")\n",
        "    students = make_students(10)\n",
        "    cls = Classroom(teacher, students, \"Defence Against the Dark Arts\")\n",
        "    new = Student(\"Harry Potter\")\n",
        "\n",
        "    cls.add_student(new)  # should not raise\n",
        "    assert any(s.name == \"Harry Potter\" for s in cls.students)\n",
        "    assert len(cls.students) == 11\n",
        "\n",
        "\n",
        "@pytest.mark.parametrize(\n",
        "    \"initial_names, remove_name, expected_remaining_names\",\n",
        "    [\n",
        "        ([\"Luna\", \"Ginny\", \"Cho\"], \"Ginny\", [\"Luna\", \"Cho\"]),                      # remove middle\n",
        "        ([\"Ron\"], \"Ron\", []),                                                      # single element removed => empty\n",
        "        ([\"Parvati\", \"Parvati\", \"Lavender\"], \"Parvati\", [\"Parvati\", \"Lavender\"]),  # duplicates: remove first\n",
        "        ([\"Fred\", \"George\"], \"George\", [\"Fred\"]),                                  # remove last\n",
        "    ],\n",
        ")\n",
        "def test_remove_student_parametrized(initial_names, remove_name, expected_remaining_names):\n",
        "    \"\"\"\n",
        "    Logic: Test remove_student behavior in several scenarios:\n",
        "      - removing existing (first occurrence removed)\n",
        "      - duplicates: only first match removed (breaks after removal)\n",
        "      - removing last / single element\n",
        "    \"\"\"\n",
        "    teacher = Teacher(\"Sybill Trelawney\")\n",
        "    students = [Student(n) for n in initial_names]\n",
        "    cls = Classroom(teacher, students, \"Divination\")\n",
        "\n",
        "    cls.remove_student(remove_name)\n",
        "    assert [s.name for s in cls.students] == expected_remaining_names\n",
        "\n",
        "\n",
        "def test_remove_student_non_existent_does_nothing():\n",
        "    \"\"\"\n",
        "    Logic: removing a name not present should leave the students list unchanged.\n",
        "    \"\"\"\n",
        "    teacher = Teacher(\"Filius Flitwick\")\n",
        "    students = make_students(3)\n",
        "    names_before = [s.name for s in students]\n",
        "    cls = Classroom(teacher, students, \"Charms\")\n",
        "\n",
        "    cls.remove_student(\"Sirius Black\")  # not present\n",
        "    assert [s.name for s in cls.students] == names_before\n",
        "\n",
        "\n",
        "def test_change_teacher_replaces_teacher_object():\n",
        "    \"\"\"\n",
        "    Logic: change_teacher should set the teacher attribute to the new object passed.\n",
        "    \"\"\"\n",
        "    old = Teacher(\"Gilderoy Lockhart\")\n",
        "    new = Teacher(\"Pomona Sprout\")\n",
        "    cls = Classroom(old, make_students(2), \"Herbology\")\n",
        "\n",
        "    cls.change_teacher(new)\n",
        "    assert cls.teacher is new\n",
        "    assert isinstance(cls.teacher, Teacher)\n",
        "\n",
        "\n",
        "@pytest.mark.parametrize(\"start_count, additions, should_raise\", [\n",
        "    (10, 1, False),  # start 10, add 1 => allowed\n",
        "    (10, 2, True),   # start 10, add 2 sequentially => second add should raise\n",
        "])\n",
        "def test_multiple_sequential_adds(start_count, additions, should_raise):\n",
        "    \"\"\"\n",
        "    Logic: Test sequential add_student calls. When starting at 10:\n",
        "      - the first add is allowed (makes 11),\n",
        "      - the second add should raise TooManyStudents.\n",
        "    We parametrize to demonstrate both passing and raising behavior.\n",
        "    \"\"\"\n",
        "    teacher = Teacher(\"Horace Slughorn\")\n",
        "    students = make_students(start_count)\n",
        "    cls = Classroom(teacher, students, \"Potions\")\n",
        "\n",
        "    # perform `additions` many adds\n",
        "    last_student = None\n",
        "    for i in range(additions):\n",
        "        new = Student(f\"New{i}\")\n",
        "        last_student = new\n",
        "        if i == additions - 1 and should_raise:\n",
        "            with pytest.raises(TooManyStudents):\n",
        "                cls.add_student(new)\n",
        "        else: cls.add_student(new)\n",
        "\n",
        "    if not should_raise:\n",
        "        assert cls.students[-1] is last_student\n",
        "        assert len(cls.students) == start_count + additions\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FnsmxX7samB",
        "outputId": "e9085bd6-4a77-4b9b-e2ae-acc4314d00fe"
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing tests/test_classroom.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the above test file:"
      ],
      "metadata": {
        "id": "d1zPWF5EvAUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest tests/test_classroom.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBBKuCCFs6jk",
        "outputId": "d87ed795-2951-4b5a-d4a3-b506d72f7f60"
      },
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.11, pytest-8.4.2, pluggy-1.6.0\n",
            "rootdir: /content/tests\n",
            "configfile: pytest.ini\n",
            "plugins: langsmith-0.4.28, typeguard-4.4.4, anyio-4.10.0\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 15 items                                                             \u001b[0m\n",
            "\n",
            "tests/test_classroom.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                  [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m============================== \u001b[32m\u001b[1m15 passed\u001b[0m\u001b[32m in 0.04s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, this is how GPT tools can be leveraged to generate test cases for your code."
      ],
      "metadata": {
        "id": "9bxcoS6hvhFr"
      }
    }
  ]
}